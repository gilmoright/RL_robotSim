# RL_robotSim
Пробую обучать RL агента на задачу следования робота за лидером в симуляциях.

# Подготовка
- Установить необходимые пакеты. Можно использовать файл environment.yml для создания окружения анаконды.
- Скачать репозиторий с виртуальной средой https://github.com/aaselivanov/continuous-grid-arctic
- Указать  путь к этому репозиторию continuous-grid-arctic в начале файла MyMisc.py и TestModel.py

# Конфиги
Конфигурации экспериментов хранятся в папке Configs. Подробнее о конфигах написано в [Configs/README.md](Configs/)

# Скрипты

## Обучение
Для запуска обучения используется скрипт MyTrain. Его параметры: 
- --config-file - путь к файлу с конфигурациями эксперимента
- --experiments - список названий экспериментов, конфигурации которых нужно взять из файла и запустить. Можно запускать одновременно несколько. Я запускал по 3 эксперимента, по 4 воркера для каждого.
```
python MyTrain.py --config-file Configs/FollowerContinuous/PPO_obst.conf --experiments ppo_env4_feats12 ppo_env4feats12_train5v2 ppo_env4feats12_train5v6
```
## Тестирование
Скрипт запускает тестирование и если надо сохранение видео для выбранной модели.
Параметры:
- rlalgo - алгоритм обучения, который использовался для получения моделей
- run_dir - пусть к результатам запуска рея, откуда будет взят конфиг и чекпоинт. Например /s/ls4/users/grartem/RL_robots/RL_robotSim/results/FollowerContinuous/env4/PPO/env4feats12_train/ppo_env4feats12_train5v6/PPO_continuous-grid_448ca_00002_2_2022-06-14_16-10-32. 
- checkpoint_number - номер чекпоинта из которого взять веса

Также перед запуском стоит задать сиды, которые будут использованы для инициализации среды и тестирования. Сиды выбирать надо вручную. перебирать разные сиды, рендерить для них картинки и выбирать, какие подойдут для тестирования. Я делал это в ноутбуке [ChoseTestExamples](Notebooks/ChoseTestExamples.ipynb).  После внесения изменений в среду, маршруты могут измениться и соответственно сиды стать недействительными. Значит при глобальных изменениях надо по новой их подбирать.
Например сейчас для среды версии env7 выбраны следующие: 
```
TEST_SEEDS = [1, 4, 5, 7 ,9, 10, 11, 14, 16, 17, 21, 24, 28, 29, 30, 32, 36, 38, 40, 43, 44, 45, 46, 49,
             #53, 58, 59, 60, 62, 63, 64, 65, 66, 70, 72, 73, 76, 82, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100,
             #103, 105, 107, 108, 109, 116,117, 119, 121,124, 127, 129, 131, 132, 138, 139, 140, 141, 146, 149, 150,
             #152, 153, 154, 155, 156, 159, 160, 161, 166, 170, 172, 173, 174, 175, 176, 177, 179, 181, 182, 183, 185, 186,
             #187, 189, 192, 194, 195, 196, 197, 198, 199]
```
Список RECORD_SEEDS указывает, для каких сидов будет сохранено видео.

Пример запуска тестирования после задания сидов: 
```
python TestModel.py --rlalgo PPO \
--run_dir $RUN_DIR \
--checkpoint_number 710
```
В результате в run_dir будет создан csv файл с отчетом по каждому сиду, как закончился запуск. И в подпапке videos будут сохранены видео.
## MyMisc
Здесь раньше делались оберётки для среды, но потом я их перенёс в репозиторий самой среды. Сейчас этот скрипт просто при импорте регистрирует среду с названием "continuous-grid", которая будет настраиваться конфигом

# Пример добавления эксперимента
Если допустим изменили что-то в среде и хотим это опробовать.
0. Если работаем с репозиторием https://github.com/aaselivanov/continuous-grid-arctic. То после того, как добавили изменения в среде, если они затронули входные признаки модели, нужно убедиться, что там есть wrapper, которй поддерживает эти изменения(continuous_grid_arctic/utils/wrappers.py). 
1. Добавить новую конфигурацию среды в файл [Configs/FollowerContinuous/Environment.conf](Configs/FollowerContinuous/Environment.conf). Если была изменена среда в репозитории continuous-grid-arctic, то менять скорее всего нужно имя зарегистрированной gym среды env_config.name, название враппера env_config.wrappers  и параметры в env_config.base_env_config. Если в среде изменилось что-то глобально (добавились другие типы препятствий, по новому строится маршрут итп), лучше инкрементировать версию в названии конфига. Сейчас последняя v7.
2. Проверить среду и отобрать сиды для тестирования. Для этого в ноутбуке [Notebooks/ChoseTestExamples.ipynb](Notebooks/ChoseTestExamples.ipynb) подставить новый конфиг среды (но выключить раннюю остановку симуляции early_stopping), запустить ~ 200 итераций с сохранением финального кадра. 
2.1 Проверить, не падает ли среда на каких-то итерациях. Если падает - фиксить, потому что рей может повиснуть на таких примерах.
2.2 Если есть возможность - сравнить со старыми сидами, если изменения в среде не затронули маршрутов и расположения препятствий, то можно использовать старые сиды из файла TestModel.py
2.3 Если нет возможности сравнить со старыми, то выбрать из сгенерированных сидов тестовые (те, которые понравятся, где ведущий не врезается сразу во что-то и маршрут не прямая линия)
3. Добавить конфигурацию нового эксперимента в [Configs/FollowerContinuous/PPO_obst.conf](Configs/FollowerContinuous/PPO_obst.conf), если планируется использовать PPO и среду с препятствиями. Или создать новый файл АЛГОРИТМ_КАКОЙТОТЭГ.conf.
4. Запустить MyTrain с указанием файла с конфигурациями и названиями запускаемых экспериментов. 
5. Запустить TestModel.py указав в скрипте нужные сидыи и в параметрах к скрипту путь к папке с результатами обучения и номер чекпоинта.

Советы.

При изменении среды рекомендуется сначала сделать небольшой запуск с 1 воркером и одним экспериментом за раз и log_level в конфиге указать DEBUG. Потому что если задать сразу несколько воркеров, при ошибке в среде, рей может просто зависнуть.

# TODO
- Организовать привязку к версиям репозиториев с виртуальными средами. Сейчас в файле MyMisc.py указывается путь к репозиторию со средой. Надо либо отдельный проект завести с submodules, либо прям сюда submodules встроить. И встроить добавление пути к репозиторию со средой в переменную окружения PATH или PYTHONPATH
- Как-то переделать фиксирование экспериментов. Потому что сиды - это костыльное решение.