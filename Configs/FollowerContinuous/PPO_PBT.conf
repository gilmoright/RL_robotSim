include "Environment_2.conf"
include "Exploration.conf"
include "Architecture.conf"
include "Architecture_2.conf"
include "Training.conf"

ppo_default {
    env = continuous-grid
    run = PPO
    local_dir = /s/ls4/users/slava1195/rl_rob/RL_robotSim/results/FollowerContinuous/dyn_obst/PPO/
    checkpoint_freq = 10
    stop {
        training_iteration = 400
    }
    config {
        num_gpus = 1
        timesteps_per_iteration = 1000
        num_workers = 4
        log_level = WARNING
        framework = torch
    }    
}



ppo_dv1 = ${ppo_default}  {
    checkpoint_freq = 5

    stop {
        training_iteration = 400
    }
    config {
        num_gpus = 1
        timesteps_per_iteration = 1000
        num_workers = 4
        log_level = WARNING
        framework = torch
    }
}
ppo_pbt_ev28_mdefprev = {
    env = continuous-grid
    run = PPO
    local_dir = /s/ls4/users/grartem/RL_robots/RL_robotSim/results/FollowerContinuous_Prev/dyn_obst/PPO/PBT
    checkpoint_freq = 10
    #10
    stop {
        training_iteration = 400
        #400
    }
    config = ${envconf_v28_b1_feats_v14v2_prev5} {
        num_workers: 3
        num_gpus = 0.5
        num_envs_per_worker = 1
        create_env_on_driver: False
        rollout_fragment_length = [tune, randint, [100, 500], 200]
        batch_mode = truncate_episodes
        gamma = [tune, uniform, [0.01, 0.999], 0.99]
        lr = [tune, uniform, [0.000001, 0.0005], 0.00001]
        train_batch_size = [tune, randint, [1025, 10000], 6000]
        # env = None
        use_critic = True
        use_gae = True
        lambda = [tune, uniform, [0, 1], 1]
        kl_coeff = [tune, uniform, [0, 1], 0.2]
        sgd_minibatch_size = [tune, randint, [32, 1024], 256]
        shuffle_sequences = True
        num_sgd_iter = [tune, randint, [1, 50], 30]
        lr_schedule = null
        vf_loss_coeff = [tune, uniform, [0, 1], 0.001]
        entropy_coeff = [tune, uniform, [0, 1], 0.0]
        entropy_coeff_schedule = null
        clip_param = [tune, uniform, [0, 0.9], 0.3]
        vf_clip_param = [tune, uniform, [0, 40], 10]
        grad_clip = null
        kl_target = [tune, uniform, [0, 0.1], 0.01]
        vf_share_layers = -1
        explore = True
        exploration_config = {
            type = [tune, choice, [Random, StochasticSampling, GaussianNoise, OrnsteinUhlenbeckNoise], StochasticSampling]
        }
        framework = tf
        model = {
            _use_default_native_models = False
            _disable_preprocessor_api = False
            fcnet_hiddens = [256, 256]
            fcnet_activation = tanh
            conv_filters = null
            conv_activation = relu
            post_fcnet_hiddens = []
            post_fcnet_activation = relu
            free_log_std = False
            no_final_linear = False
            vf_share_layers = False
            use_lstm = False
            max_seq_len = 20
            lstm_cell_size = 256
            lstm_use_prev_action = False
            lstm_use_prev_reward = False
            _time_major = False
            use_attention = False
            attention_num_transformer_units = 1
            attention_dim = 64
            attention_num_heads = 1
            attention_head_dim = 32
            attention_memory_inference = 50
            attention_memory_training = 50
            attention_position_wise_mlp_dim = 32
            attention_init_gru_gate_bias = 2.0
            attention_use_n_prev_actions = 0
            attention_use_n_prev_rewards = 0
            framestack = True
            dim = 84
            grayscale = False
            zero_mean = True
            custom_model = def_m_prev
            custom_action_dist = null
            custom_preprocessor = null
            lstm_use_prev_action_reward = -1
        }
        horizon = null
        soft_horizon = False
        no_done_at_end = False
        observation_space = null
        action_space = null
        remote_worker_envs = False
        remote_env_batch_wait_ms = 0
        env_task_fn = null
        render_env = False
        record_env = False
        clip_rewards = null
        normalize_actions = True
        clip_actions = False
        preprocessor_pref = deepmind
        log_level = WARNING
        #callbacks = ray.rllib.agents.callbacks.DefaultCallbacks
        ignore_worker_failures = False
        log_sys_usage = True
        fake_sampler = False
        eager_tracing = False
        eager_max_retraces = 20
        evaluation_interval = null
        evaluation_num_episodes = 10
        evaluation_parallel_to_training = False
        in_evaluation = False
        evaluation_num_workers = 0
        custom_eval_function = null
        sample_async = False
        #sample_collector = ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector
        observation_filter = NoFilter
        synchronize_filters = True
        tf_session_args = {
            intra_op_parallelism_threads = 2
            inter_op_parallelism_threads = 2
            gpu_options.allow_growth = True
            log_device_placement = False
            device_count.CPU = 1
            allow_soft_placement = True
        }
        local_tf_session_args = {
            intra_op_parallelism_threads = 8
            inter_op_parallelism_threads = 8
        }        
        compress_observations = False
        collect_metrics_timeout = 180
        metrics_smoothing_episodes = 100
        min_iter_time_s = 0
        timesteps_per_iteration = 0
        seed = null
        _fake_gpus = False
        num_cpus_per_worker = 1
        num_gpus_per_worker = 0
        num_cpus_for_driver = 1
        placement_strategy = PACK
        input = sampler
        actions_in_input_normalized = False
        input_evaluation = ["is","wis"]
        postprocess_inputs = False
        shuffle_buffer_size = 0
        output = null
        output_compress_columns = ["obs","new_obs"]
        output_max_file_size = 67108864
        multiagent = {
            policy_map_capacity = 100
            policy_map_cache = null
            policy_mapping_fn = null
            policies_to_train = null
            observation_fn = null
            replay_mode =independent
            count_steps_by =env_steps
        }
        logger_config = null
        _tf_policy_handles_more_than_one_loss = False
        _disable_preprocessor_api = False
        simple_optimizer = -1
        monitor = -1
    }
}