arch_default {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = relu
    }
}


arch_default_ac {
    critic_hiddens = [400, 300]
    actor_hiddens = [400, 300]
}

arch_v1 {
    model = {
        fcnet_hiddens = [8, 8]
        fcnet_activation = relu
    }
}
arch_v1_ac = ${arch_v1} {
    critic_hiddens = [8, 8]
    actor_hiddens = [8, 8]
}

arch_v1v2_ac = {
    model = {
        fcnet_hiddens = [8, 8]
        fcnet_activation = tanh
    }
    critic_hiddens = [8, 8]
    actor_hiddens = [8, 8]
}

arch_v1_lstm {
    model = {
        fcnet_hiddens = [8, 8]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 8
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 100
    }    
}

arch_v2 {
    model = {
        fcnet_hiddens = [16,16]
        fcnet_activation = relu
    }
}
arch_v2_ac = ${arch_v2} {
    critic_hiddens = [16, 16]
    actor_hiddens = [16, 16]
}
arch_v2v2_ac = {
    model = {
        fcnet_hiddens = [16, 16]
        fcnet_activation = tanh
    }
    critic_hiddens = [16, 16]
    actor_hiddens = [16, 16]
}

arch_v3 {
    model = {
        fcnet_hiddens = [32,32]
        fcnet_activation = relu
    }
}

arch_v3_ac = ${arch_v3} {
    critic_hiddens = [32, 32]
    actor_hiddens = [32, 32]
}
arch_v3_lstm {
    model = {
        fcnet_hiddens = [32, 32]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 32
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 100
    }    
}
arch_v3_lstm2 = ${arch_v3_lstm} {
    model = {
        max_seq_len = 20
    }    
}
arch_v3_lstm3 = ${arch_v3_lstm} {
    model = {
        max_seq_len = 20
        lstm_use_prev_reward = False
        lstm_use_prev_action = False
    }
}
arch_v3v2_ac = {
    model = {
        fcnet_hiddens = [32, 32]
        fcnet_activation = tanh
    }
    critic_hiddens = [32, 32]
    actor_hiddens = [32, 32]
}

arch_v4 {
    model = {
        fcnet_hiddens = [64,64]
        fcnet_activation = relu
    }
}
arch_v4_ac = ${arch_v4} {
    critic_hiddens = [64, 64]
    actor_hiddens = [64, 64]
}
arch_v4v2_ac = {
    model = {
        fcnet_hiddens = [64, 64]
        fcnet_activation = tanh
    }
    critic_hiddens = [64, 64]
    actor_hiddens = [64, 64]
}
arch_v4_lstm {
    model = {
        fcnet_hiddens = [64, 64]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 64
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 100
    }    
}

arch_v5 {
    model = {
        fcnet_hiddens = [128,128]
        fcnet_activation = relu
    }
}
arch_v5_ac = ${arch_v5} {
    critic_hiddens = [128, 128]
    actor_hiddens = [128, 128]
}
arch_v5v2_ac = {
    model = {
        fcnet_hiddens = [128, 128]
        fcnet_activation = tanh
    }
    critic_hiddens = [128, 128]
    actor_hiddens = [128, 128]
}
arch_v5_lstm {
    model = {
        fcnet_hiddens = [128, 128]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 128
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 100
    }    
}

arch_v6 {
    model = {
        fcnet_hiddens = [256,256]
        fcnet_activation = relu
    }
}
arch_v6_ac = ${arch_v6} {
    critic_hiddens = [256, 256]
    actor_hiddens = [256, 256]
}
arch_v6v2_ac = {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = tanh
    }
    critic_hiddens = [256, 256]
    actor_hiddens = [256, 256]
}
arch_v6_lstm {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 256
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 100
    }
}

##############################
### NEW ARC with LSTM
##############################
arch_v7_lstm {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 256
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 20
    }
}

arch_v7_lstmv1 {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 256
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 5
    }
}
arch_v7_lstmv2 {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 256
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 10
    }
}
#########################
arch_v8_lstmv1 {
    model = {
        fcnet_hiddens = [128, 128]
        fcnet_activation = relu
        use_lstm = True
        lstm_cell_size = 128
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 20
    }
}

arch_v8_lstmv2 {
    model = {
        fcnet_hiddens = [128, 128]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 128
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 20
    }
}


arch_v9_lstmv1 {
    model = {
        fcnet_hiddens = [64, 64]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 64
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 10
    }
}

######################################

arch_v8 {
    model = {
        fcnet_hiddens = [128, 128]
        fcnet_activation = tanh
    }
}
###############################
arch_v10_lstm_v1 {
    model = {
        fcnet_hiddens = [32, 32]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 32
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 50
    }
}

arch_v10_lstm_v2 {
    model = {
        fcnet_hiddens = [32, 32]
        fcnet_activation = tanh
        use_lstm = True
        lstm_cell_size = 32
        lstm_use_prev_action = True
        lstm_use_prev_reward = True
        max_seq_len = 20
    }
}
############################################

arch_d_v1 {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = relu
    }
}

arch_d_v2 {
    model = {
        fcnet_hiddens = [512, 512]
        fcnet_activation = relu
    }
}

arch_d_v1_th {
    model = {
        fcnet_hiddens = [256, 256]
        fcnet_activation = tanh
    }
}

arch_d_v2_th {
    model = {
        fcnet_hiddens = [512, 512]
        fcnet_activation = tanh
    }
}

######################################

my_transformer_model {
    framework = "tf"
    model = {
        "custom_model": "transformer_model"
    }
}


my_transformer_model_v2 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v2"
    }
}

my_transformer_model_v3 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v3"
    }
}

my_transformer_model_v4 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v4"
        custom_model_config = {
            head_size = 128
            dropout = 0.05
            num_heads=4
            transformer_blocks_count = 1
            flattening_type = "Flatten"
        }        
    }
}
my_transformer_model_v4v2 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v4"
        custom_model_config = {
            head_size = 256
            dropout = 0.2
            num_heads=4
            transformer_blocks_count = 1
            flattening_type = "Flatten"
        }
    }
}
my_transformer_model_v4v3 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v4"
        custom_model_config = {
            head_size = 256
            dropout = 0.2
            num_heads=4
            transformer_blocks_count = 1
            flattening_type = "GlobalAveragePooling1D"
        }
    }
}
my_transformer_model_v4v4 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v4"
        custom_model_config = {
            head_size = 64
            dropout = 0.05
            num_heads=4
            transformer_blocks_count = 4
            flattening_type = "Flatten"
        }        
    }
}
my_transformer_model_v4v5 {
    framework = "tf"
    model = {
        "custom_model": "transformer_model_v4"
        custom_model_config = {
            head_size = 256
            dropout = 0.2
            num_heads=4
            transformer_blocks_count = 1
            flattening_type = "LastObs"
        }
    }
}

my_model_v1 {
    framework = "tf"
    model = {
        "custom_model": "my_model_v1"
    }
}

# Для этой архитектуры в Ray ray/rllib/models/torch/attention_net.py в 380 строке стоит добавить клип. потому что трансформеры почему-то слишком большие значения могут возвращать
model_raytrans_v1 {
    model = {
        max_seq_len = 5
        use_attention = True
        attention_num_transformer_units = 1
        attention_dim = 64
        attention_num_heads = 1
        attention_head_dim = 32
        attention_memory_inference = 5
        attention_memory_training = 5
        attention_position_wise_mlp_dim = 32
        attention_init_gru_gate_bias = 2.0
        attention_use_n_prev_actions = 5
        attention_use_n_prev_rewards = 5
    }
}